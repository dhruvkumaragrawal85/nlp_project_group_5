{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11388464,"sourceType":"datasetVersion","datasetId":7131611}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\nimport nltk\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom transformers import pipeline\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:44:59.009888Z","iopub.execute_input":"2025-04-14T07:44:59.010709Z","iopub.status.idle":"2025-04-14T07:45:00.156601Z","shell.execute_reply.started":"2025-04-14T07:44:59.010681Z","shell.execute_reply":"2025-04-14T07:45:00.155872Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def load_dataset(file_path):\n    with open(file_path, \"r\") as f:\n        data = json.load(f)\n    return data\n\ndata = load_dataset(\"/kaggle/input/dataset/dailydialog_test.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:45:00.157828Z","iopub.execute_input":"2025-04-14T07:45:00.158255Z","iopub.status.idle":"2025-04-14T07:45:00.188402Z","shell.execute_reply.started":"2025-04-14T07:45:00.158236Z","shell.execute_reply":"2025-04-14T07:45:00.187870Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Your input data should already be defined as 'data'\n\ndef process_data(data):\n    records = []\n    for conversation_id, conversation in data.items():\n        for dialogue in conversation:\n            for entry in dialogue:\n                records.append({\n                    \"conversation_id\": conversation_id,\n                    \"turn\": entry[\"turn\"],\n                    \"speaker\": entry[\"speaker\"],\n                    \"utterance\": entry[\"utterance\"],\n                    \"emotion\": entry.get(\"emotion\", \"neutral\"),\n                    \"cause_evidence\": entry.get(\"expanded emotion cause span\", [])\n                })\n    return pd.DataFrame(records)\n\n# Abbreviations we want to protect\nprotected_abbreviations = [\"Mr .\", \"Mrs .\", \"Ms .\", \"Dr .\", \"Prof .\", \"Sr .\", \"Jr .\", \"St .\", \"vs .\"]\n\n# Replace protected abbreviations with temporary placeholders\ndef replace_abbreviations(text, reverse=False):\n    if not reverse:\n        for i, abbr in enumerate(protected_abbreviations):\n            text = text.replace(abbr, f\"__ABBR{i}__\")\n    else:\n        for i, abbr in enumerate(protected_abbreviations):\n            text = text.replace(f\"__ABBR{i}__\", abbr)\n    return text\n\ndef split_into_clauses(text):\n    text = replace_abbreviations(text)  # Protect abbreviations\n    clauses = re.split(r'[.?!,;:]', text)  # Safe regex now\n    clauses = [clause.strip() for clause in clauses if clause.strip()]\n    clauses = [replace_abbreviations(clause, reverse=True) for clause in clauses]  # Restore abbreviations\n    return clauses\n\n# Process and transform dataset\ndf = process_data(data)\ndf['clauses'] = df['utterance'].apply(split_into_clauses)\n\n# Save processed DataFrame to JSON file\ndf.to_json(\"processed_data.json\", orient=\"records\", indent=2)\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:45:00.188978Z","iopub.execute_input":"2025-04-14T07:45:00.189146Z","iopub.status.idle":"2025-04-14T07:45:00.245978Z","shell.execute_reply.started":"2025-04-14T07:45:00.189132Z","shell.execute_reply":"2025-04-14T07:45:00.245461Z"}},"outputs":[{"name":"stdout","text":"  conversation_id  turn speaker  \\\n0         tr_9708     1       A   \n1         tr_9708     2       B   \n2         tr_9708     3       A   \n3         tr_9708     4       B   \n4         tr_9708     5       A   \n\n                                           utterance   emotion  \\\n0                         The blake's got divorced .   neutral   \n1                                     Really ? Why ?   neutral   \n2  Mr . black has been getting a little around as...   neutral   \n3  I'm surprised . He does't look like a guy who'...  surprise   \n4  No , he doesn't . But his wife found out he ha...   neutral   \n\n                                      cause_evidence  \\\n0                                                 []   \n1                                                 []   \n2                                                 []   \n3  [Mr . black has been getting a little around a...   \n4                                                 []   \n\n                                             clauses  \n0                         [The blake's got divorced]  \n1                                      [Really, Why]  \n2  [Mr . black has been getting a little around a...  \n3  [I'm surprised, He does't look like a guy who'...  \n4  [No, he doesn't, But his wife found out he has...  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df.to_json(\"processed_data.json\", orient=\"records\", indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:45:00.247219Z","iopub.execute_input":"2025-04-14T07:45:00.247450Z","iopub.status.idle":"2025-04-14T07:45:00.257652Z","shell.execute_reply.started":"2025-04-14T07:45:00.247434Z","shell.execute_reply":"2025-04-14T07:45:00.257128Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nfrom transformers import pipeline\n\n# Define labels\nLABELS = [\"emotion clause\", \"cause clause\", \"neutral clause\"]\n\n# Load zero-shot classification pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-base-zeroshot-v1\")\n\n# Expanded emotion keywords\nemotion_keywords = set([\n    \"happy\", \"joyful\", \"joy\", \"sad\", \"angry\", \"surprised\", \"afraid\", \"disgusted\", \"anxious\", \"excited\",\n    \"pleased\", \"delighted\", \"ecstatic\", \"elated\", \"thrilled\", \"content\",\n    \"depressed\", \"unhappy\", \"upset\", \"miserable\", \"down\", \"gloomy\",\n    \"furious\", \"irritated\", \"frustrated\", \"mad\", \"outraged\",\n    \"shocked\", \"astonished\", \"amazed\", \"startled\", \"stunned\",\n    \"scared\", \"fearful\", \"terrified\", \"nervous\", \"worried\", \"panicked\",\n    \"disgusted\", \"repulsed\", \"nauseated\",\n    \"hopeful\", \"grateful\", \"thankful\", \"lonely\", \"ashamed\", \"guilty\",\n    \"embarrassed\", \"confused\", \"jealous\", \"envious\", \"proud\", \"relieved\",\n    \"i'm happy\", \"i feel great\", \"i'm upset\", \"i'm worried\", \"i'm scared\",\n    \"i'm nervous\", \"i feel bad\", \"i'm thankful\", \"i'm shocked\", \"i'm sad\",\n    \"i feel embarrassed\", \"i feel guilty\", \"i'm afraid\"\n])\n\ndef contains_emotion_keywords(clause):\n    clause_lower = clause.lower()\n    return any(word in clause_lower for word in emotion_keywords)\n\ndef classify_clause_with_hf(utterance, clause, emotion):\n    # Emotion override\n    if contains_emotion_keywords(clause):\n        return \"emotion_clause\"\n\n    # Zero-shot classification\n    hypothesis = \"This clause functions as an {} in the context of emotion in dialogue.\"\n    result = classifier(clause, LABELS, hypothesis_template=hypothesis)\n    label_scores = dict(zip(result[\"labels\"], result[\"scores\"]))\n\n    if label_scores[\"emotion clause\"] >= 0.5:\n        return \"emotion_clause\"\n    elif label_scores[\"cause clause\"] >= 0.3:\n        return \"cause_clause\"\n    else:\n        return \"neutral_clause\"\n\ndef process_dataset(data):\n    results = {}\n\n    for item in tqdm(data, desc=\"Processing\"):\n        conv_id = item[\"conversation_id\"]\n        if conv_id not in results:\n            results[conv_id] = []\n\n        utterance = item.get(\"utterance\", \"\")\n        emotion = item.get(\"emotion\", \"unknown\")\n        clauses = item.get(\"clauses\", [])\n        cause_spans = item.get(\"cause_evidence\", [])\n\n        classified_clauses = []\n        emotion_clauses = []\n        cause_clauses = []\n\n        for clause in clauses:\n            label = classify_clause_with_hf(utterance, clause, emotion)\n            if label == \"emotion_clause\":\n                emotion_clauses.append(clause)\n            elif label == \"cause_clause\":\n                cause_clauses.append(clause)\n\n            classified_clauses.append({\"clause\": clause, \"label\": label})\n\n        linked_emotions = []\n        for e_clause in emotion_clauses:\n            linked_causes = [cause for cause in cause_spans if cause in utterance]\n            linked_emotions.append({\"emotion_clause\": e_clause, \"caused_by\": linked_causes})\n\n        results[conv_id].append({\n            \"turn\": item[\"turn\"],\n            \"speaker\": item[\"speaker\"],\n            \"utterance\": utterance,\n            \"emotion\": emotion,\n            \"clauses\": classified_clauses,\n            \"emotion_cause_mapping\": linked_emotions\n        })\n\n    return results\n\nif __name__ == \"__main__\":\n    input_path = \"processed_data.json\"\n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        input_data = json.load(f)\n\n    results = process_dataset(input_data)\n\n    with open(\"hf_results_final.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4, ensure_ascii=False)\n\n    print(\"ðŸŽ‰ Classification results saved to hf_results_final.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T07:45:00.258391Z","iopub.execute_input":"2025-04-14T07:45:00.258620Z","iopub.status.idle":"2025-04-14T07:50:58.750450Z","shell.execute_reply.started":"2025-04-14T07:45:00.258594Z","shell.execute_reply":"2025-04-14T07:50:58.749807Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nProcessing:   0%|          | 4/2405 [00:02<16:16,  2.46it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nProcessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2405/2405 [05:57<00:00,  6.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸŽ‰ Classification results saved to hf_results_final.json\n","output_type":"stream"}],"execution_count":6}]}